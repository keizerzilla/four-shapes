{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hu Moments for Image Recognition: Intro\n",
    "\n",
    "Moments are very common features extracted from images to be used in pattern recogntion tasks, such as face recognition and shape retrieval. In this short paper, let's take a look at Hu Moments, undoubtedly the most important work of this domain. The main reference is the work **Visual Pattern Recognition by Moment Invariants** wrote by MK Hu in [1962](https://ieeexplore.ieee.org/document/1057692). Both the math and Python code are provided to better understanding of the subject, alongside more detailed explanations. Please feel free to help, share and improve this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Moments\n",
    "\n",
    "We define _Image Moments_ as **the weighted average of all pixel intensities of a image**. Consider a binary image described by the function $I(x, y)$ with dimenisons $NxN$ pixels (any size is possible, not just square ones), we can calculate the raw moments using:\n",
    "\n",
    "$$M_{pq} = \\sum_{x=0}^{N-1}\\sum_{y=0}^{N-1}x^py^qI(x, y)$$\n",
    "\n",
    "The above expression shows a summation of all pixel intensities pondered by its location $(x, y)$ over the powers $p$ and $q$. In other words, image moments are values that carry both spatial and intensity information, e.g, **shape**. $p$ and $q$ are the weights of the horizontal and vertical dimensions, respectivelly. The sum $p+q$ is the _moment order_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid Localization\n",
    "\n",
    "We can use the raw moments to extract important information of an image. By doing $M_{00}$, we are accumulating the non-zero intensities. It's like describing the spatial information of the pixel \"blob\". Similary, doing for the $X$ and $Y$ dimensions ($M_{10}$ and $M_{01}$), one can pinpoint the centroid coordinates $(\\bar{x}, \\bar{y})$ of the blob by doing:\n",
    "\n",
    "$$\\bar{x} = \\frac{M_{10}}{M_{00}}$$\n",
    "$$\\bar{y} = \\frac{M_{01}}{M_{00}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Invariance\n",
    "\n",
    "The centroid can be used to rewrite the raw moment equation to achieve the **translation invariant** momento $\\mu_{pq}$:\n",
    "\n",
    "$$\\mu_{pq} = \\sum_{x=0}^{N-1}\\sum_{y=0}^{N-1}(x - \\bar{x})^p(y - \\bar{y})^qI(x, y)$$\n",
    "\n",
    "Now, the relative spatial information of the centroid is being take in consideration, so no matter where the blob is localized the moments will be (roughly) the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Invariance\n",
    "\n",
    "Scaling (change of size) is another very common transformation performed in images. This scaling can be uniform (the same in both dimensions) or non-uniform. Hu showed that you can relate the zero order translate invariant moment to get scale invariants $\\eta_{pq}$:\n",
    "\n",
    "$$\\eta_{pq} = \\frac{\\mu_{pq}}{\\mu_{00}^{1 + \\frac{p+q}{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hu Moments\n",
    "\n",
    "We call **Hu Moments** the set of 7 values propesed by Hu in his 1962 work _Visual Pattern Recognition by Moment Invariants_:\n",
    "\n",
    "$h_1 = \\eta_{20} + \\eta_{02}$\n",
    "\n",
    "$h_2 = (\\eta_{20} - \\eta_{02})^2 + 4(\\eta_{11})^2$\n",
    "\n",
    "$h_3 = (\\eta_{30} - 3\\eta_{12})^2 + 3(\\eta_{03} - 3\\eta_{21})^2$\n",
    "\n",
    "$h_4 = (\\eta_{30} + \\eta_{12})^2 + (\\eta_{03} + \\eta_{21})^2$\n",
    "\n",
    "$h_5 = (\\eta_{30} - 3\\eta_{12})(\\eta_{30} + \\eta_{12})[(\\eta_{30} + \\eta_{12})^2 - 3(\\eta_{03} + \\eta_{21})^2] + (3\\eta_{21} - \\eta_{03})(\\eta_{03} + \\eta_{21})[3(\\eta_{30} + \\eta_{12})^2 - (\\eta_{03} + \\eta_{21})^2]$\n",
    "\n",
    "$h_6 = (\\eta_{20} - \\eta_{02})[(\\eta_{30} + \\eta_{12})^2 - 7(\\eta_{03} + \\eta_{21})^2] + 4\\eta_{11}(\\eta_{30} + \\eta_{12})(\\eta_{03} + \\eta_{21})$\n",
    "\n",
    "$h_7 = (3\\eta_{21} - \\eta_{03})(\\eta_{30} + \\eta_{12})[(\\eta_{30} + \\eta_{12})^2 - 3(\\eta_{03} + \\eta_{21})^2] + (\\eta_{30} - 3\\eta_{12})(\\eta_{03} + \\eta_{21})[3(\\eta_{30} + \\eta_{12})^2 - (\\eta_{03} + \\eta_{21})^2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: The Four Shapes Dataset\n",
    "\n",
    "To illustrate the use of Hu Moments in pattern recognition, we will use the **Four Shapes Dataset**. This dataset contains 14970 samples of four classes: circles, squares, stars and triangles. You can get a free copy of the data in [Kaggle](https://www.kaggle.com/smeschke/four-shapes). We created the bellow animations using the first 400 samples of each class to showcase the diversity of examples:\n",
    "\n",
    "| ![SegmentLocal](images/circles.gif \"segment\") | ![SegmentLocal](images/squares.gif \"segment\") | ![SegmentLocal](images/stars.gif \"segment\") | ![SegmentLocal](images/triangles.gif \"segment\") |\n",
    "|-----------------------------------------------|-----------------------------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "\n",
    "After downloading and extracting the contents, the data is organized in a folder called shapes with four subfolders, one for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Code\n",
    "\n",
    "The code is written in Python 3.x and makes use of the following packages: ```opencv-python```, ```scikit-learn```, ```numpy```, ```pandas``` and ```matplotlib```. First, let's import the all the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.neighbors import NearestCentroid as NC\n",
    "from sklearn.model_selection import train_test_split as data_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two dictionaries. The first represents the relation between the class name literal and a numerical label. The second, the set of classifiers chosen for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "\t\"circle\"   : 1,\n",
    "\t\"square\"   : 2,\n",
    "\t\"star\"     : 3,\n",
    "\t\"triangle\" : 4\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "\t\"NC\"         : NC(),\n",
    "\t\"LDA\"        : LDA(),\n",
    "\t\"QDA\"        : QDA(),\n",
    "\t\"SVM_linear\" : SVM(kernel=\"linear\"),\n",
    "\t\"SVM_radial\" : SVM(kernel=\"rbf\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function we gonna create is responsible to extract the Moments of all samples, storing the class label as a eight column and save everything in a ```CSV``` file. Also, a simple logarithmic transformation is performed to equalize the orders of the moments, wich is a good pre-processing step when handling this type of feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data_file, dataset=\"shapes/\"):\n",
    "\tdump = []\n",
    "\t\n",
    "\tprint(\"Extracting Hu moments...\")\n",
    "\t\n",
    "\tfor c, idx in classes.items():\n",
    "\t\tclass_folder = dataset + \"{}/\".format(c)\n",
    "\t\t\n",
    "\t\tfor f in os.listdir(class_folder):\n",
    "\t\t\tfpath = class_folder + f\n",
    "\t\t\tsample = int(f.replace(\".png\", \"\"))\n",
    "\t\t\t\n",
    "\t\t\timg = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)\n",
    "\t\t\timg = cv2.bitwise_not(img)\n",
    "\t\t\thu = cv2.HuMoments(cv2.moments(img))\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(0, 7):\n",
    "\t\t\t\thu[i] = -1 * np.sign(hu[i]) * np.log10(np.abs(hu[i]))\n",
    "\t\t\t\n",
    "\t\t\thu = hu.reshape((1, 7)).tolist()[0] + [sample, idx]\n",
    "\t\t\tdump.append(hu)\n",
    "\t\t\n",
    "\t\tprint(c, \"ok!\")\n",
    "\n",
    "\tcols = [\"hu1\", \"hu2\", \"hu3\", \"hu4\", \"hu5\", \"hu6\", \"hu7\", \"sample\", \"class\"]\n",
    "\t\n",
    "\tdf = pd.DataFrame(dump, columns=cols)\n",
    "\tdf.to_csv(data_file, index=None)\n",
    "\t\n",
    "\tprint(\"Extraction done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing this function and asking to store the results in ```hu_moments.csv```, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Hu moments...\n",
      "circle ok!\n",
      "square ok!\n",
      "star ok!\n",
      "triangle ok!\n",
      "Extraction done!\n"
     ]
    }
   ],
   "source": [
    "data_file = \"hu_moments.csv\"\n",
    "\n",
    "feature_extraction(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function performs the classification. The feature vectors were split in training and test sets with 70/30 proportion, respectively, with a default number of test iterations set to 100. This function returns a dataframe with the results of each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(data_file, rounds=100, remove_disperse=[]):\n",
    "\tdf = pd.read_csv(data_file)\n",
    "\tdf = df.drop([\"sample\"], axis=1)\n",
    "\t\n",
    "\tif remove_disperse:\n",
    "\t\tdf = df.drop(remove_disperse, axis=1)\n",
    "\t\n",
    "\tX = df.drop([\"class\"], axis=1)\n",
    "\ty = df[\"class\"]\n",
    "\t\n",
    "\tans = {key: {\"score\" : [], \"sens\" : [], \"spec\" : []}\n",
    "\t       for key, value in classifiers.items()}\n",
    "\t\n",
    "\tprint(\"Classifying...\")\n",
    "\t\n",
    "\tfor i in range(rounds):\n",
    "\t\tX_train, X_test, y_train, y_test = data_split(X, y, test_size=0.3)\n",
    "\t\t\n",
    "\t\tfor name, classifier in classifiers.items():\n",
    "\t\t\tscaler = StandardScaler()\n",
    "\t\t\tscaler.fit(X_train)\n",
    "\t\t\tX_train = scaler.transform(X_train)\n",
    "\t\t\tX_test = scaler.transform(X_test)\n",
    "\t\t\t\n",
    "\t\t\tclassifier.fit(X_train, y_train)\n",
    "\t\t\tscore = classifier.score(X_test, y_test)\n",
    "\t\t\t\n",
    "\t\t\tans[name][\"score\"].append(score)\n",
    "\t\t\n",
    "\tprint(\"Classification done!\")\n",
    "\t\n",
    "\treturn ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line executes it and stores the result in ```ans```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying...\n",
      "Classification done!\n"
     ]
    }
   ],
   "source": [
    "ans = classification(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the classification performance in a more frendly way using the ```summary()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumary(ans, title=\"Summary\"):\n",
    "\tsize = 70\n",
    "\tseparator = \"-\"\n",
    "\t\n",
    "\tprint(separator*size)\n",
    "\tprint(\"SUMARY: {}\".format(title))\n",
    "\tprint(separator*size)\n",
    "\tprint(\"CLASSIF\\t\\tMEAN\\tMEDIAN\\tMINV\\tMAXV\\tSTD\")\n",
    "\tprint(separator*size)\n",
    "\t\n",
    "\tfor n in ans:\n",
    "\t\tm = round(np.mean(ans[n][\"score\"])*100, 2)\n",
    "\t\tmed = round(np.median(ans[n][\"score\"])*100, 2)\n",
    "\t\tminv = round(np.min(ans[n][\"score\"])*100, 2)\n",
    "\t\tmaxv = round(np.max(ans[n][\"score\"])*100, 2)\n",
    "\t\tstd = round(np.std(ans[n][\"score\"])*100, 2)\n",
    "\t\t\n",
    "\t\tprint(\"{:<16}{}\\t{}\\t{}\\t{}\\t{}\".format(n, m, med, minv, maxv, std))\n",
    "\t\n",
    "\tprint(separator*size)\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "SUMARY: Summary\n",
      "----------------------------------------------------------------------\n",
      "CLASSIF\t\tMEAN\tMEDIAN\tMINV\tMAXV\tSTD\n",
      "----------------------------------------------------------------------\n",
      "NC              96.52\t96.5\t95.93\t97.19\t0.26\n",
      "LDA             99.49\t99.49\t99.18\t99.64\t0.09\n",
      "QDA             99.83\t99.84\t99.67\t99.96\t0.06\n",
      "SVM_linear      99.92\t99.91\t99.8\t100.0\t0.04\n",
      "SVM_radial      99.98\t99.98\t99.91\t100.0\t0.02\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sumary(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As we can see, all classifiers performed almost perfectly in this task, with the only exception being the NC. The SVM classifier configured with Linear Kernel had the highest mean accuracy and lowest standard deviation, making it the best choice in this simple application. With simple math involved and a rather fast extraction/classification time, the Hu Moments are a good benchmark for visual pattern recognition, as well as a good entry example for those who are starting with computer vision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
